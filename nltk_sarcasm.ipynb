{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, CuDNNGRU, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "#######\n",
    "# Load data\n",
    "#######\n",
    "fp = 'Sarcasm_Headlines_Dataset.json'\n",
    "\n",
    "def load_data(fp):\n",
    "    # Return data as list of list, first element of each data point is\n",
    "    # headline, second is 0/1 indicator for is_sarcastic\n",
    "    with open(fp, 'r') as f:\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(line) for line in data]\n",
    "        return np.array([[row['headline'], row['is_sarcastic']] for row in data])\n",
    "\n",
    "data = load_data(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7984525146636715\n",
      "Positive class ratio is: 0.44128291526269814\n",
      "GS best score: 0.7968014548566539\n",
      "GS best params: {'clf__alpha': 1.0, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# Bernoulli model (naive bayes)\n",
    "########\n",
    "\n",
    "class bernoulli_model():\n",
    "    def __init__(self, data):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "            train_test_split(data[:,0], data[:,1], test_size=0.30, random_state=20)\n",
    "        self.sarcasm_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                                ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                                ('clf', BernoulliNB())])\n",
    "\n",
    "    def model_results(self):\n",
    "        sarcasm_clf = self.sarcasm_clf.fit(self.X_train, self.y_train)\n",
    "        predictions = sarcasm_clf.predict(self.X_test)\n",
    "        print('Accuracy is:', np.mean(predictions == self.y_test))\n",
    "        print('Positive class ratio is:', np.mean(self.y_test == '1'))\n",
    "\n",
    "    def model_GS(self):\n",
    "        parameters = {'vect__ngram_range': [(1, 1), (1, 2), (1,3)],\n",
    "                        'tfidf__use_idf': (True, False),\n",
    "                        'clf__alpha': (1.0, .1, .01, )}\n",
    "\n",
    "        gs_clf = GridSearchCV(self.sarcasm_clf, parameters, n_jobs=-1, cv=5)\n",
    "        gs_clf = gs_clf.fit(self.X_train, self.y_train)\n",
    "        print('GS best score:', gs_clf.best_score_)\n",
    "        print('GS best params:', gs_clf.best_params_)\n",
    "\n",
    "m1_bernoulli = bernoulli_model(data)\n",
    "m1_bernoulli.model_results()\n",
    "m1_bernoulli.model_GS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.7855984025957818\n",
      "Positive class ratio is: 0.44128291526269814\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# Logistic regression model\n",
    "########\n",
    "\n",
    "class logistic():\n",
    "    def __init__(self, data):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "            train_test_split(data[:,0], data[:,1], test_size=0.30, random_state=20)\n",
    "        self.sarcasm_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                                ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                                ('clf', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "    def model_results(self):\n",
    "        sarcasm_clf = self.sarcasm_clf.fit(self.X_train, self.y_train)\n",
    "        predictions = sarcasm_clf.predict(self.X_test)\n",
    "        print('Accuracy is:', np.mean(predictions == self.y_test))\n",
    "        print('Positive class ratio is:', np.mean(self.y_test == '1'))\n",
    "\n",
    "m2_logistic = logistic(data)\n",
    "m2_logistic.model_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14956 samples, validate on 3740 samples\n",
      "Epoch 1/3\n",
      "14956/14956 [==============================] - 10s 645us/step - loss: 0.6371 - acc: 0.6220 - val_loss: 0.5316 - val_acc: 0.7366\n",
      "Epoch 2/3\n",
      "14956/14956 [==============================] - 6s 399us/step - loss: 0.3574 - acc: 0.8536 - val_loss: 0.4526 - val_acc: 0.7952\n",
      "Epoch 3/3\n",
      "14956/14956 [==============================] - 6s 399us/step - loss: 0.1927 - acc: 0.9293 - val_loss: 0.5104 - val_acc: 0.7890\n",
      "Evaluating model against test set:\n",
      "8013/8013 [==============================] - 4s 457us/step\n",
      "[0.47915095404770497, 0.8003244727466878]\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# Using empty word embedding and LSTM\n",
    "########\n",
    "\n",
    "class LSTM_model():\n",
    "    def __init__(self, data):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "                    train_test_split(data[:,0], data[:,1], test_size=0.30, random_state=20)\n",
    "        self.y_train, self.y_test = list(map(int, self.y_train)), list(map(int, self.y_test))\n",
    "        self.model = None\n",
    "\n",
    "    def run_model(self):\n",
    "        def clean_sen(sen):\n",
    "            tokens = word_tokenize(sen)\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            words = [w for w in words if not w in stop_words]\n",
    "\n",
    "            return ' '.join(words)\n",
    "\n",
    "        self.X_train = list(map(lambda x: clean_sen(x), self.X_train))\n",
    "        self.X_test = list(map(lambda x: clean_sen(x), self.X_test))\n",
    "\n",
    "        all_data = self.X_train + self.X_test\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(all_data)\n",
    "\n",
    "        # Max num of words in headline\n",
    "        max_len = max([len(s.split()) for s in all_data])\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        X_train_tokens = tokenizer.texts_to_sequences(self.X_train)\n",
    "        X_test_tokens = tokenizer.texts_to_sequences(self.X_test)\n",
    "\n",
    "        X_train_pad = pad_sequences(X_train_tokens, maxlen=max_len)\n",
    "        X_test_pad = pad_sequences(X_test_tokens, maxlen=max_len)\n",
    "\n",
    "        EMBEDDING_DIMS = 50\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(vocab_size, EMBEDDING_DIMS, input_length=max_len))\n",
    "        self.model.add(LSTM(units=20, dropout=0.2, recurrent_dropout=0.2))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.model.fit(X_train_pad, y_train, batch_size=128, epochs=3, validation_split=0.2)\n",
    "        print(\"Evaluating model against test set:\")\n",
    "        print(self.model.evaluate(X_test_pad, self.y_test))\n",
    "        print(self.model.metrics_names)\n",
    "\n",
    "m3_LSTM = LSTM_model(data)\n",
    "m3_LSTM.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14956 samples, validate on 3740 samples\n",
      "Epoch 1/3\n",
      "14956/14956 [==============================] - 8s 519us/step - loss: 0.5247 - acc: 0.7351 - val_loss: 0.4584 - val_acc: 0.7794\n",
      "Epoch 2/3\n",
      "14956/14956 [==============================] - 5s 302us/step - loss: 0.3191 - acc: 0.8639 - val_loss: 0.4318 - val_acc: 0.8094\n",
      "Epoch 3/3\n",
      "14956/14956 [==============================] - 4s 298us/step - loss: 0.1722 - acc: 0.9340 - val_loss: 0.4986 - val_acc: 0.8061\n",
      "Evaluating model against test set:\n",
      "8013/8013 [==============================] - 1s 103us/step\n",
      "[0.47489634850610435, 0.8109322351551258]\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# Using pre-trained word vector and 2 stacked GRUs\n",
    "# #######\n",
    "\n",
    "class GRU_model():\n",
    "    def __init__(self, data):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "                    train_test_split(data[:,0], data[:,1], test_size=0.30, random_state=20)\n",
    "        self.y_train, self.y_test = list(map(int, self.y_train)), list(map(int, self.y_test))\n",
    "        self.model = None\n",
    "\n",
    "    def run_model(self):\n",
    "        def clean_sen(sen):\n",
    "            tokens = word_tokenize(sen)\n",
    "            tokens = [w.lower() for w in tokens]\n",
    "\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            stripped = [w.translate(table) for w in tokens]\n",
    "            words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            words = [w for w in words if not w in stop_words]\n",
    "\n",
    "            return ' '.join(words)\n",
    "\n",
    "        self.X_train = list(map(lambda x: clean_sen(x), self.X_train))\n",
    "        self.X_test = list(map(lambda x: clean_sen(x), self.X_test))\n",
    "\n",
    "        # Max num of words in headline\n",
    "        max_len = max([len(s.split()) for s in all_data])\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        X_train_tokens = tokenizer.texts_to_sequences(self.X_train)\n",
    "        X_test_tokens = tokenizer.texts_to_sequences(self.X_test)\n",
    "\n",
    "        X_train_pad = pad_sequences(X_train_tokens, maxlen=max_len)\n",
    "        X_test_pad = pad_sequences(X_test_tokens, maxlen=max_len)\n",
    "\n",
    "        pretrained_embedding = api.load(\"glove-wiki-gigaword-100\")\n",
    "        word_index = tokenizer.word_index\n",
    "        num_words = len(word_index) + 1\n",
    "\n",
    "        embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "        for word, index in word_index.items():\n",
    "            if word not in pretrained_embedding:\n",
    "                continue\n",
    "            embedding_vector = pretrained_embedding[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "\n",
    "        # create model\n",
    "        self.model = Sequential()\n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                    100, \n",
    "                                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                                    input_length=max_len,\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.model.add(embedding_layer)\n",
    "        self.model.add(CuDNNGRU(units=20, return_sequences=True))\n",
    "        self.model.add(CuDNNGRU(units=20))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.model.fit(X_train_pad, y_train, validation_split=0.2, epochs = 3)\n",
    "        print(\"Evaluating model against test set:\")\n",
    "        print(self.model.evaluate(X_test_pad, self.y_test))\n",
    "        print(self.model.metrics_names)\n",
    "\n",
    "m4_GRU = GRU_model(data)\n",
    "m4_GRU.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
